{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if not './' in sys.path:\n",
    "    sys.path.append('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from envs.stocks_env_multiaction import Stocks_env\n",
    "from datasets import nyse\n",
    "from models.lstm import ActorCritic\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use GPU</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "WARNING:tensorflow:From <ipython-input-3-94fe1c854373>:14: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "              tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "tf.test.is_gpu_available()\n",
    "\n",
    "# set up policy used in mixed precision\n",
    "# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = nyse.load_data('../data/')\n",
    "data, tokenized_industry, vocabulary_size = nyse.load_data_with_industry('../data/', column='GICS Sector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GAE and PPO</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):        \n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states_daily, states_industry, actions, log_probs, returns, advantage):\n",
    "    batch_size = tf.shape(states_daily)[0]\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states_daily.numpy()[rand_ids, :], np.array(states_industry)[rand_ids, :], actions.numpy()[rand_ids, :], log_probs.numpy()[rand_ids, :], returns.numpy()[rand_ids, :], advantage.numpy()[rand_ids, :]\n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states_daily, states_industry, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state_daily, state_industry, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states_daily, states_industry, actions, log_probs, returns, advantages):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # value, dist = model(state, np.reshape(state[:,-1,:],(batch_size, 1, num_inputs[1])))\n",
    "                value, dist = model(state_daily, state_industry)\n",
    "\n",
    "                entropy = tf.math.reduce_mean(dist.entropy())\n",
    "                new_log_probs = dist.log_prob(action)\n",
    "\n",
    "                ratio = tf.math.exp(new_log_probs - old_log_probs)\n",
    "                surr1 = ratio * advantage\n",
    "                surr2 = tf.clip_by_value(ratio, clip_value_min=1.0 - clip_param, \n",
    "                                         clip_value_max=1.0 + clip_param) * advantage\n",
    "\n",
    "                actor_loss  = - tf.math.reduce_mean(tf.math.minimum(surr1, surr2))\n",
    "                critic_loss = tf.math.reduce_mean(tf.math.pow(return_ - value, 2))\n",
    "\n",
    "                loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training Parameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper params:\n",
    "lr               = 1e-5\n",
    "run_lenght       = 10\n",
    "batch_size       = 256\n",
    "window_size      = 5\n",
    "ppo_epochs       = 4\n",
    "num_epochs       = 800\n",
    "test_iterations  = 1\n",
    "hidden_dim       = 512\n",
    "kernel_size      = 9\n",
    "num_filters      = 128\n",
    "lstm_units       = 1024\n",
    "num_blocks       = 1\n",
    "embedding_out    = 6\n",
    "in_lstm_units    = 16\n",
    "initial_money    = 100\n",
    "test_seed        = 42\n",
    "model_filename   = None\n",
    "with_hold        = False\n",
    "\n",
    "\n",
    "# log\n",
    "log_freq = 10\n",
    "save_freq = 40\n",
    "models_directory = 'results/models/'\n",
    "save_directory = 'results/saved-timesteps/'\n",
    "date = datetime.now().strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "identifier = \"lstm-\" + date\n",
    "test_summary_writer = tf.summary.create_file_writer('results/summaries/test/' + identifier)\n",
    "mean_test_reward = tf.keras.metrics.Mean(name='mean_test_reward')\n",
    "mean_train_reward = tf.keras.metrics.Mean(name='mean_train_reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Enviroment</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize env\n",
    "env = Stocks_env(data, window_size, run_lenght, batch_size=batch_size, with_hold=with_hold,\n",
    "                 tokenized_industry=tokenized_industry, test_seed=test_seed, initial_money=initial_money)\n",
    "\n",
    "# test parameters\n",
    "test_batch_size  = len(env.get_test_symbols())\n",
    "unique_symbols = data.symbol.unique()\n",
    "mini = 999\n",
    "for symbol in unique_symbols:\n",
    "    if len(data[data.symbol==symbol]) < mini:\n",
    "        mini = len(data[data.symbol==symbol])\n",
    "# test_run_lenght = run_lenght\n",
    "test_run_lenght = mini-window_size-1\n",
    "test_initial_money = initial_money\n",
    "# test_batch_size  = batch_size\n",
    "\n",
    "# inputs and policies\n",
    "num_inputs  = env.get_observation_space()\n",
    "num_policies = env.get_action_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = ActorCritic(num_policies = num_policies, hidden_dim=hidden_dim, num_filters=num_filters, \n",
    "                    lstm_units=lstm_units, text_lenght=tokenized_industry.shape[1],\n",
    "                    vocabulary_size=vocabulary_size, embedding_out=embedding_out, in_lstm_units = in_lstm_units)\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "if model_filename:\n",
    "    state = env.reset()\n",
    "    model(state[0], state[1])\n",
    "    model.load_weights(models_directory + model_filename + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(epochs, rewards_train, rewards_test):\n",
    "    clear_output(True)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,5))\n",
    "    fig.suptitle('frame %s. ' % epochs[-1])\n",
    "    ax1.set_title('train. reward: %s' % rewards_train[-1])\n",
    "    ax1.plot(epochs, rewards_train)\n",
    "    ax2.set_title('test. reward: %s' % rewards_test[-1])\n",
    "    ax2.plot(epochs, rewards_test)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    plt.show()\n",
    "    return image\n",
    "\n",
    "def test_env(record_days=False, random_reset=True, run_lenght=20):\n",
    "    state = env.reset(training=False, batch_size=test_batch_size, run_lenght=run_lenght, \n",
    "                      initial_money=test_initial_money, random_reset=random_reset)\n",
    "    done = False\n",
    "    operation_array = []\n",
    "    days_array = []\n",
    "    rewards_array = []\n",
    "    total_profit = np.zeros(test_batch_size)\n",
    "    while not done:\n",
    "        _, dist = model(state[0], state[1])\n",
    "        next_state, reward, done, operations, day, profit = env.step(dist.sample())\n",
    "        state = next_state\n",
    "        if record_days:\n",
    "            operation_array.append(np.array(operations))\n",
    "            days_array.append(np.array(day))\n",
    "            rewards_array.append(np.array(reward))\n",
    "        mean_test_reward(np.array(reward))\n",
    "        total_profit += profit\n",
    "    total_profit = total_profit/test_initial_money\n",
    "    return operation_array, days_array, rewards_array, total_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (256,) (71,) (256,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-97f9892eac94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             operation_array, days_array, rewards_array, test_total_profit = test_env(\n\u001b[0;32m---> 67\u001b[0;31m                 record_days=(i==test_iterations-1), random_reset=False, run_lenght=run_lenght)\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mtotal_profits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_profit\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0minitial_money\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mtest_total_profits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_total_profit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ee9b054826d8>\u001b[0m in \u001b[0;36mtest_env\u001b[0;34m(record_days, random_reset, run_lenght)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mrewards_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mmean_test_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtotal_profit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprofit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mtotal_profit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_profit\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest_initial_money\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moperation_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdays_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_profit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (256,) (71,) (256,) "
     ]
    }
   ],
   "source": [
    "train_rewards = []\n",
    "test_rewards = []\n",
    "total_profits = []\n",
    "test_total_profits = []\n",
    "best_test = 0\n",
    "epoch  = 0\n",
    "epochs = []\n",
    "\n",
    "start = time.time()\n",
    "while epoch < num_epochs:\n",
    "\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states_daily = []\n",
    "    states_industry = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    total_profit = np.zeros(batch_size)\n",
    "\n",
    "    state = env.reset(training=True, batch_size=batch_size, run_lenght=run_lenght, initial_money=initial_money)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        value, dist = model(state[0], state[1])\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _, _, profit = env.step(action)\n",
    "        mean_train_reward(reward)\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(np.array(value))\n",
    "        rewards.append(tf.dtypes.cast(tf.expand_dims((reward,), axis=1), tf.float32))\n",
    "        masks.append(tf.expand_dims((float(1 - done),), axis=1))\n",
    "        \n",
    "        total_profit += profit\n",
    "\n",
    "        states_daily.append(state[0])\n",
    "        states_industry.append(state[1])\n",
    "        actions.append(action)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # next_value, _ = model(next_state, np.reshape(next_state[:,-1,:],(batch_size, 1, num_inputs[1])))\n",
    "    next_value, _ = model(next_state[0], next_state[1])\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns_save = returns\n",
    "\n",
    "    returns   = tf.reshape(tf.concat(returns, axis=1),(run_lenght*batch_size,-1))\n",
    "    log_probs = tf.concat(log_probs, axis=0)\n",
    "    values    = tf.concat(values, axis=0)\n",
    "    states_daily    = tf.concat(states_daily, axis=0)    \n",
    "    states_industry = tf.reshape(states_industry,(run_lenght*batch_size,-1))\n",
    "    actions   = tf.reshape(tf.concat(actions, axis=0),(run_lenght*batch_size,-1))\n",
    "\n",
    "    advantage = returns - values\n",
    "    advantage = tf.reshape(advantage,(run_lenght*batch_size**2,1))\n",
    "\n",
    "    ppo_update(ppo_epochs, batch_size, states_daily, states_industry, actions, log_probs, returns, advantage)\n",
    "\n",
    "    if epoch % log_freq == 0:\n",
    "        for i in range(test_iterations):\n",
    "            operation_array, days_array, rewards_array, test_total_profit = test_env(\n",
    "                record_days=(i==test_iterations-1), random_reset=False, run_lenght=run_lenght)\n",
    "        total_profits.append(total_profit/initial_money)\n",
    "        test_total_profits.append(test_total_profit)\n",
    "        train_rewards.append(mean_train_reward.result().numpy())\n",
    "        test_rewards.append(mean_test_reward.result().numpy())\n",
    "        epochs.append(epoch)\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('mean_test_reward', mean_test_reward.result(), step=epoch)\n",
    "            tf.summary.scalar('mean_train_reward', mean_train_reward.result(), step=epoch)\n",
    "            tf.summary.image('Plot', plot(epochs, train_rewards, test_rewards), step=epoch)\n",
    "        # serialize weights to HDF5\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        if not os.path.exists(save_directory+'/operation/'):\n",
    "            os.makedirs(save_directory+'/operation/')\n",
    "        if not os.path.exists(save_directory+'/endingday/'):\n",
    "            os.makedirs(save_directory+'/endingday/')\n",
    "        if not os.path.exists(save_directory+'/rewards/'):\n",
    "            os.makedirs(save_directory+'/rewards/')\n",
    "        if not os.path.exists(save_directory+'/profits/'):\n",
    "            os.makedirs(save_directory+'/profits/')\n",
    "        if not os.path.exists(save_directory+'/test-profits/'):\n",
    "            os.makedirs(save_directory+'/test-profits/')\n",
    "        pd.DataFrame(operation_array).to_csv(save_directory+\"/operation/{}-epoch{}.csv\".format(identifier, epoch), \n",
    "                                             header=env.get_current_symbols(), index=None)\n",
    "        pd.DataFrame(days_array).to_csv(save_directory+\"/endingday/{}-epoch{}.csv\".format(identifier, epoch), \n",
    "                                             header=env.get_current_symbols(), index=None)\n",
    "        pd.DataFrame(rewards_array).to_csv(save_directory+\"/rewards/{}-epoch{}.csv\".format(identifier, epoch), \n",
    "                                             header=env.get_current_symbols(), index=None)\n",
    "        pd.DataFrame(total_profits).to_csv(save_directory+\"/profits/{}.csv\".format(identifier),\n",
    "                                           index=None)\n",
    "        pd.DataFrame(test_total_profits).to_csv(save_directory+\"/test-profits/{}.csv\".format(identifier),\n",
    "                                                index=None)\n",
    "        \n",
    "        if epoch % save_freq == 0:\n",
    "            if not os.path.exists(models_directory):\n",
    "                os.makedirs(models_directory)\n",
    "            if best_test<mean_test_reward.result():\n",
    "                model.save_weights(models_directory + \"best-{}.h5\".format(identifier))\n",
    "            model.save_weights(models_directory + \"model-{}.h5\".format(identifier))\n",
    "\n",
    "        mean_test_reward.reset_states()\n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "        start = time.time()\n",
    "\n",
    "    mean_train_reward.reset_states()\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = 'results/test-all/'\n",
    "models_directory = 'results/models/'\n",
    "test_summary_writer = tf.summary.create_file_writer('results/summaries/test/' + identifier)\n",
    "mean_test_reward = tf.keras.metrics.Mean(name='mean_test_reward')\n",
    "train_test_ratio=0.2\n",
    "\n",
    "model.load_weights(models_directory + \"best-{}.h5\".format(identifier[10:]))\n",
    "\n",
    "env = Stocks_env(data, window_size, run_lenght, random_reset=True, train_test_ratio=train_test_ratio,\n",
    "                 tokenized_industry=tokenized_industry, test_seed=test_seed, initial_money=initial_money)\n",
    "\n",
    "repeat = 100\n",
    "\n",
    "test_total_profits = []\n",
    "\n",
    "for i in range(repeat):\n",
    "\n",
    "    operation_array, days_array, rewards_array, test_total_profit = test_env(record_days=True, random_reset=True,\n",
    "                                                                            run_lenght=test_run_lenght)\n",
    "    test_total_profits.append(test_total_profit)\n",
    "\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('mean_test_reward', mean_test_reward.result(), step=i)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "    if not os.path.exists(save_directory+'operations/'):\n",
    "        os.makedirs(save_directory+'operations/')\n",
    "    if not os.path.exists(save_directory+'endingdays/'):\n",
    "        os.makedirs(save_directory+'endingdays/')\n",
    "    if not os.path.exists(save_directory+'rewards/'):\n",
    "        os.makedirs(save_directory+'rewards/')\n",
    "    if not os.path.exists(save_directory+'profits/'):\n",
    "        os.makedirs(save_directory+'profits/')\n",
    "    pd.DataFrame(operation_array).to_csv(save_directory+\"operations/{}-iteration{}.csv\".format(identifier, i), \n",
    "                                         header=env.get_current_symbols(), index=None)\n",
    "    pd.DataFrame(days_array).to_csv(save_directory+\"endingdays/{}-iteration{}.csv\".format(identifier, i), \n",
    "                                         header=env.get_current_symbols(), index=None)\n",
    "    pd.DataFrame(rewards_array).to_csv(save_directory+\"rewards/{}-iteration{}.csv\".format(identifier, i), \n",
    "                                         header=env.get_current_symbols(), index=None)\n",
    "    pd.DataFrame(test_total_profits).to_csv(save_directory+\"profits/{}.csv\".format(identifier),\n",
    "                                            index=None)\n",
    "    mean_test_reward.reset_states()\n",
    "\n",
    "    print(\"{}: {}\".format(i, np.mean(test_total_profits)/test_run_lenght*30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
